{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fb6fb7-743a-4afe-b315-7c584e368366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install all necessary packages in one cell\n",
    "%pip install -U mlflow databricks-openai databricks-agents tabulate\n",
    "\n",
    "# Restart Python to ensure all new packages are available\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf310b45-665e-40dd-bc63-bfd753c6ef0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def nimble_health_providers(city: str, display: bool = False):\n",
    "    df = spark.sql(f\"SELECT * FROM `dais-hackathon-2025`.nimble.dbx_google_maps_search_daily where array_contains(business_category, 'Health')\")\n",
    "    pdf = df.toPandas()\n",
    "    pdf = pdf[pdf.city == city]\n",
    "    # stringify arrays and complex data\n",
    "    for col in pdf.columns:\n",
    "        if pdf[col].apply(lambda x: isinstance(x, (list, tuple, np.ndarray))).any():\n",
    "            pdf[col] = pdf[col].apply(str)\n",
    "    # display\n",
    "    if display:\n",
    "        df.show()\n",
    "    # return\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6aa885e-e853-412a-994f-b95f98fcbf68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Consolidated code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ebc2d4e-af5d-4f5d-a00f-33fe8c82c6c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%%writefile quickstart_agent.py\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import ChatAgentMessage, ChatAgentResponse, ChatContext\n",
    "\n",
    "import mlflow\n",
    "import json\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_openai import UCFunctionToolkit, DatabricksFunctionClient\n",
    "\n",
    "import uuid\n",
    "from typing import Any, Optional\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# MODEL_NAME = \"databricks-claude-sonnet-4\"\n",
    "# MODEL_NAME = \"databricks-claude-3-5-sonnet\",\n",
    "MODEL_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "\n",
    "# Get an OpenAI client configured to connect to Databricks model serving endpoints\n",
    "# Use this client to query the LLM\n",
    "openai_client = WorkspaceClient().serving_endpoints.get_open_ai_client()\n",
    "\n",
    "# Enable automatic tracing for easier debugging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Load Databricks built-in tools (Python code interpreter)\n",
    "client = DatabricksFunctionClient()\n",
    "\n",
    "class ToolResult:\n",
    "  def __init__(self, value):\n",
    "    self.value = value\n",
    "\n",
    "tools = []\n",
    "\n",
    "# weather\n",
    "\n",
    "def get_weather(city: str) -> ToolResult:\n",
    "  if city == \"Tokyo\":\n",
    "    value = \"sunny\"\n",
    "  elif city == \"Paris\":\n",
    "    value = \"rainy\"\n",
    "  else:\n",
    "    value = \"unknown\"\n",
    "  return ToolResult(value)\n",
    "weather_tool = {\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"city\": {\"type\": \"string\"}\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "tools.append(weather_tool)\n",
    "\n",
    "# health data\n",
    "def get_health_provider_data(city: str) -> ToolResult:\n",
    "  pdf = nimble_health_providers(city, True)\n",
    "  for col in pdf.columns:\n",
    "      if pdf[col].apply(lambda x: isinstance(x, (list, tuple, np.ndarray))).any():\n",
    "          pdf[col] = pdf[col].apply(str)\n",
    "  pdf_markdown_str = pdf.to_markdown(index=False)\n",
    "  return ToolResult(pdf_markdown_str)\n",
    "#\n",
    "get_health_provider_data_tool = {\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"get_health_provider_data\",\n",
    "    \"parameters\": {\n",
    "      \"city\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The city for which to retrieve health provider data.\"\n",
    "      }\n",
    "    },\n",
    "  }\n",
    "}\n",
    "tools.append(get_health_provider_data_tool)\n",
    "\n",
    "\n",
    "def call_tool(tool_name, parameters):\n",
    "  if tool_name == \"get_weather\":\n",
    "    return get_weather(**parameters)\n",
    "  elif tool_name == \"get_health_provider_data\":\n",
    "    return get_health_provider_data(**parameters)\n",
    "  raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "\n",
    "# ...\n",
    "def addToolResultToPrompt(prompt, tool_name, tool_result_value):\n",
    "  return (\n",
    "    f\"{prompt}\\n\\n\"\n",
    "    f\"NOTE: The tool '{tool_name}' was run and returned the following result: {tool_result_value}\\n\"\n",
    "    \"Please use this result to answer the user's question.\"\n",
    "  )\n",
    "\n",
    "# ...\n",
    "def run_agent(prompt):\n",
    "  \"\"\"\n",
    "  Send a user prompt to the LLM and return a list of LLM response messages\n",
    "  The LLM is allowed to call the code interpreter tool, if needed, to respond to the user\n",
    "  \"\"\"\n",
    "\n",
    "  system_prompt = (\n",
    "    \"You are an assistant that can use external tools. \"\n",
    "    \"If a tool result is available, always use the tool result to answer the user's question. \"\n",
    "    \"If no tool result is available, proceed as usual.\"\n",
    "  )\n",
    "\n",
    "  toolResultPresent = False\n",
    "  while True:\n",
    "    result_msgs = []\n",
    "    msgs = [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    # only use tools on the first call\n",
    "    if toolResultPresent:\n",
    "      insertTools = []\n",
    "    else:\n",
    "      insertTools = tools\n",
    "    # call\n",
    "    response = openai_client.chat.completions.create(\n",
    "      model=MODEL_NAME,\n",
    "      messages=msgs,\n",
    "      tools=insertTools,\n",
    "    )\n",
    "    msg = response.choices[0].message\n",
    "    result_msgs.append(msg.to_dict())\n",
    "    # If the model executed a tool, get the result\n",
    "    if msg.tool_calls:\n",
    "      call = msg.tool_calls[0]\n",
    "      tool_result = call_tool(call.function.name, json.loads(call.function.arguments))\n",
    "      prompt = addToolResultToPrompt(prompt, call.function.name, tool_result.value)\n",
    "      toolResultPresent = True\n",
    "      continue\n",
    "    # return after getting a result from the LLM assistant\n",
    "    break\n",
    "  return result_msgs\n",
    "\n",
    "\n",
    "# comment for testing\n",
    "class QuickstartAgent(ChatAgent):\n",
    "  def predict(\n",
    "    self,\n",
    "    messages: list[ChatAgentMessage],\n",
    "    context: Optional[ChatContext] = None,\n",
    "    custom_inputs: Optional[dict[str, Any]] = None,\n",
    "  ) -> ChatAgentResponse:\n",
    "    # 1. Extract the last user prompt from the input messages\n",
    "    prompt = messages[-1].content\n",
    "\n",
    "    # 2. Call run_agent to get back a list of response messages\n",
    "    raw_msgs = run_agent(prompt)\n",
    "\n",
    "    # 3. Map each response message into a ChatAgentMessage and return\n",
    "    # the response\n",
    "    out = []\n",
    "    for m in raw_msgs:\n",
    "      out.append(ChatAgentMessage(\n",
    "        id=uuid.uuid4().hex,\n",
    "        **m\n",
    "      ))\n",
    "\n",
    "    return ChatAgentResponse(messages=out)\n",
    "\n",
    "AGENT = QuickstartAgent()\n",
    "mlflow.models.set_model(AGENT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42e2dbb-af98-44af-ae3e-be9125e4bedb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Log agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2a946ba-6305-46c8-9db7-659cc32dc5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "MODEL_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "CATALOG_NAME = \"team12a\"\n",
    "\n",
    "# Change the catalog name (\"main\") and schema name (\"default\") to register the agent to a different location\n",
    "registered_model_name = f\"{CATALOG_NAME}.default.quickstart_agent\"\n",
    "\n",
    "# Specify Databricks resources that the agent needs to access.\n",
    "# This step lets Databricks automatically configure authentication\n",
    "# so the agent can access these resources when it's deployed.\n",
    "resources = [\n",
    "  DatabricksServingEndpoint(endpoint_name=MODEL_NAME),\n",
    "]\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "  \n",
    "logged_agent_info = mlflow.pyfunc.log_model(\n",
    "  artifact_path=\"agent\",\n",
    "  python_model=\"quickstart_agent.py\",\n",
    "  extra_pip_requirements=[f\"databricks-connect=={get_distribution('databricks-connect').version}\"],\n",
    "  resources=resources,\n",
    "  registered_model_name=registered_model_name\n",
    ")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44bb4088-0c92-4818-8e02-c883d2c29204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Deploy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e5f48d-830c-46ec-8be1-dad64c234bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "deployment_info = agents.deploy(\n",
    "  model_name=registered_model_name, model_version=logged_agent_info.registered_model_version\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "zach - agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
